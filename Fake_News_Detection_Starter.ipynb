{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rahul21388/Fake-News-Detection/blob/main/Fake_News_Detection_Starter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhR-ZUkwJrFn"
      },
      "source": [
        "# Fake News Detection\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXeHUV5fJGiZ"
      },
      "source": [
        "## Objective\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kX73YZdgJIgF"
      },
      "source": [
        "The objective of this assignment is to develop a Semantic Classification model. You will be using Word2Vec method to extract the semantic relations from the text and develop a basic understanding of how to train supervised models to categorise text based on its meaning, rather than just syntax. You will explore how this technique is used in situations where understanding textual meaning plays a critical role in making accurate and efficient decisions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Business Objective\n",
        "\n",
        "The spread of fake news has become a significant challenge in today’s digital world. With the massive volume of news articles published daily, it’s becoming harder to distinguish between credible and misleading information. This creates a need for systems that can automatically classify news articles as true or fake, helping to reduce misinformation and protect public trust.\n",
        "\n",
        "\n",
        "In this assignment, you will develop a Semantic Classification model that uses the Word2Vec method to detect recurring patterns and themes in news articles. Using supervised learning models, the goal is to build a system that classifies news articles as either fake or true.\n"
      ],
      "metadata": {
        "id": "Gg_J6K8Xxfk2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySqxOckxI4-F"
      },
      "source": [
        "<h2> Pipelines that needs to be performed </h2>\n",
        "\n",
        "You need to perform the following tasks to complete the assignment:\n",
        "\n",
        "<ol type=\"1\">\n",
        "\n",
        "  <li> Data Preparation\n",
        "  <li> Text Preprocessing\n",
        "  <li> Train Validation Split\n",
        "  <li> EDA on Training Data\n",
        "  <li> EDA on Validation Data [Optional]\n",
        "  <li> Feature Extraction\n",
        "  <li> Model Training and Evaluation\n",
        "\n",
        "</ol>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTxV-3GJUhWm"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofebI8ITG-Li"
      },
      "source": [
        "**NOTE:** The marks given along with headings and sub-headings are cumulative marks for those particular headings/sub-headings.<br>\n",
        "\n",
        "The actual marks for each task are specified within the tasks themselves.\n",
        "\n",
        "For example, marks given with heading *2* or sub-heading *2.1* are the cumulative marks, for your reference only. <br>\n",
        "\n",
        "The marks you will receive for completing tasks are given with the tasks.\n",
        "\n",
        "Suppose the marks for two tasks are: 3 marks for 2.1.1 and 2 marks for 3.2.2, or\n",
        "* 2.1.1 [3 marks]\n",
        "* 3.2.2 [2 marks]\n",
        "\n",
        "then, you will earn 3 marks for completing task 2.1.1 and 2 marks for completing task 3.2.2.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdQjht7dUiHt"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b1lNTpKF54T"
      },
      "source": [
        "## Data Dictionary\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43j_A_GsI9TS"
      },
      "source": [
        "For this assignment, you will work with two datasets, `True.csv` and `Fake.csv`.\n",
        "Both datasets contain three columns:\n",
        "<ul>\n",
        "  <li> title of the news article\n",
        "  <li> text of the news article\n",
        "  <li> date of article publication\n",
        "</ul>\n",
        "\n",
        "`True.csv` dataset includes 21,417 true news, while the `Fake.csv` dataset comprises 23,502 fake news."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_oTQwQ_Rh4nT"
      },
      "source": [
        "## Installing required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lIY57QOLiCA2",
        "scrolled": true,
        "outputId": "d16cd387-c9dc-4642-ab8e-6461d7d14502",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.26.4\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m876.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.4\n",
            "Requirement already satisfied: pandas==2.2.2 in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas==2.2.2) (1.17.0)\n",
            "Requirement already satisfied: nltk==3.9.1 in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk==3.9.1) (8.2.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk==3.9.1) (1.5.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk==3.9.1) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk==3.9.1) (4.67.1)\n",
            "Collecting spacy==3.7.5\n",
            "  Downloading spacy-3.7.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (27 kB)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy==3.7.5) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy==3.7.5) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy==3.7.5) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy==3.7.5) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy==3.7.5) (3.0.9)\n",
            "Collecting thinc<8.3.0,>=8.2.2 (from spacy==3.7.5)\n",
            "  Downloading thinc-8.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy==3.7.5) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy==3.7.5) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy==3.7.5) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy==3.7.5) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy==3.7.5) (0.15.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy==3.7.5) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy==3.7.5) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy==3.7.5) (2.11.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy==3.7.5) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy==3.7.5) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy==3.7.5) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy==3.7.5) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy==3.7.5) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy==3.7.5) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.7.5) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.7.5) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.7.5) (4.13.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.7.5) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.7.5) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.7.5) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.7.5) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.7.5) (2025.4.26)\n",
            "Collecting blis<0.8.0,>=0.7.8 (from thinc<8.3.0,>=8.2.2->spacy==3.7.5)\n",
            "  Downloading blis-0.7.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy==3.7.5) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy==3.7.5) (8.2.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy==3.7.5) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy==3.7.5) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy==3.7.5) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy==3.7.5) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy==3.7.5) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy==3.7.5) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy==3.7.5) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy==3.7.5) (2.19.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy==3.7.5) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy==3.7.5) (0.1.2)\n",
            "Downloading spacy-3.7.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading thinc-8.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (920 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m920.2/920.2 kB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading blis-0.7.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m108.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: blis, thinc, spacy\n",
            "  Attempting uninstall: blis\n",
            "    Found existing installation: blis 1.3.0\n",
            "    Uninstalling blis-1.3.0:\n",
            "      Successfully uninstalled blis-1.3.0\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.3.6\n",
            "    Uninstalling thinc-8.3.6:\n",
            "      Successfully uninstalled thinc-8.3.6\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.8.5\n",
            "    Uninstalling spacy-3.8.5:\n",
            "      Successfully uninstalled spacy-3.8.5\n",
            "Successfully installed blis-0.7.11 spacy-3.7.5 thinc-8.2.5\n",
            "Collecting scipy==1.12\n",
            "  Downloading scipy-1.12.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<1.29.0,>=1.22.4 in /usr/local/lib/python3.11/dist-packages (from scipy==1.12) (1.26.4)\n",
            "Downloading scipy-1.12.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.4/38.4 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scipy\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.15.3\n",
            "    Uninstalling scipy-1.15.3:\n",
            "      Successfully uninstalled scipy-1.15.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed scipy-1.12.0\n",
            "Collecting pydantic==2.10.5\n",
            "  Downloading pydantic-2.10.5-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic==2.10.5) (0.7.0)\n",
            "Collecting pydantic-core==2.27.2 (from pydantic==2.10.5)\n",
            "  Downloading pydantic_core-2.27.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic==2.10.5) (4.13.2)\n",
            "Downloading pydantic-2.10.5-py3-none-any.whl (431 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.4/431.4 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_core-2.27.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydantic-core, pydantic\n",
            "  Attempting uninstall: pydantic-core\n",
            "    Found existing installation: pydantic_core 2.33.2\n",
            "    Uninstalling pydantic_core-2.33.2:\n",
            "      Successfully uninstalled pydantic_core-2.33.2\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.11.4\n",
            "    Uninstalling pydantic-2.11.4:\n",
            "      Successfully uninstalled pydantic-2.11.4\n",
            "Successfully installed pydantic-2.10.5 pydantic-core-2.27.2\n",
            "Requirement already satisfied: wordcloud==1.9.4 in /usr/local/lib/python3.11/dist-packages (1.9.4)\n",
            "Requirement already satisfied: numpy>=1.6.1 in /usr/local/lib/python3.11/dist-packages (from wordcloud==1.9.4) (1.26.4)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from wordcloud==1.9.4) (11.2.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from wordcloud==1.9.4) (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud==1.9.4) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud==1.9.4) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud==1.9.4) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud==1.9.4) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud==1.9.4) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud==1.9.4) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud==1.9.4) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->wordcloud==1.9.4) (1.17.0)\n",
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.11/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.15.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.10.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2025.4.26)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.19.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "Installing collected packages: en-core-web-sm\n",
            "  Attempting uninstall: en-core-web-sm\n",
            "    Found existing installation: en_core_web_sm 3.8.0\n",
            "    Uninstalling en_core_web_sm-3.8.0:\n",
            "      Successfully uninstalled en_core_web_sm-3.8.0\n",
            "Successfully installed en-core-web-sm-3.7.1\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade numpy==1.26.4\n",
        "!pip install --upgrade pandas==2.2.2\n",
        "!pip install --upgrade nltk==3.9.1\n",
        "!pip install --upgrade spacy==3.7.5\n",
        "!pip install --upgrade scipy==1.12\n",
        "!pip install --upgrade pydantic==2.10.5\n",
        "!pip install wordcloud==1.9.4\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuLFIymAL58u"
      },
      "source": [
        "## Importing the necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "O-Q9pqrcJrFr"
      },
      "outputs": [],
      "source": [
        "# Import essential libraries for data manipulation and analysis\n",
        "import numpy as np  # For numerical operations and arrays\n",
        "import pandas as pd  # For working with dataframes and structured data\n",
        "import re  # For regular expression operations (text processing)\n",
        "import nltk  # Natural Language Toolkit for text processing\n",
        "import spacy  # For advanced NLP tasks\n",
        "import string  # For handling string-related operations\n",
        "\n",
        "# Optional: Uncomment the line below to enable GPU support for spaCy (if you have a compatible GPU)\n",
        "#spacy.require_gpu()\n",
        "\n",
        "# Load the spaCy small English language model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# For data visualization\n",
        "import seaborn as sns  # Data visualization library for statistical graphics\n",
        "import matplotlib.pyplot as plt  # Matplotlib for creating static plots\n",
        "# Configure Matplotlib to display plots inline in Jupyter Notebook\n",
        "%matplotlib inline\n",
        "\n",
        "# Suppress unnecessary warnings to keep output clean\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# For interactive plots\n",
        "from plotly.offline import plot  # Enables offline plotting with Plotly\n",
        "import plotly.graph_objects as go  # For creating customizable Plotly plots\n",
        "import plotly.express as px  # A high-level interface for Plotly\n",
        "\n",
        "# For preprocessing and feature extraction in machine learning\n",
        "from sklearn.feature_extraction.text import (  # Methods for text vectorization\n",
        "    CountVectorizer,  # Converts text into a bag-of-words model\n",
        ")\n",
        "\n",
        "# Import accuracy, precision, recall, f_score from sklearn to predict train accuracy\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Pretty printing for better readability of output\n",
        "from pprint import pprint\n",
        "\n",
        "# For progress tracking in loops (useful for larger datasets)\n",
        "from tqdm import tqdm, tqdm_notebook  # Progress bar for loops\n",
        "tqdm.pandas()  # Enables progress bars for pandas operations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "s8Le3OfjI666"
      },
      "outputs": [],
      "source": [
        "## Change the display properties of pandas to max\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtRLCsNVJrFt"
      },
      "source": [
        "## Load the data\n",
        "\n",
        "Load the True.csv and Fake.csv files as DataFrames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "puVzIf_iJrFt"
      },
      "outputs": [],
      "source": [
        "# Import the first file - True.csv\n",
        "\n",
        "# Import the second file - Fake.csv\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xYpH-sAJrFu"
      },
      "source": [
        "## **1.** Data Preparation  <font color = red>[10 marks]</font>\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2fTeYJImEv7"
      },
      "source": [
        "### **1.0** Data Understanding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lf8ufHH5JrFu"
      },
      "outputs": [],
      "source": [
        "# Inspect the DataFrame with True News to understand the given data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gI7X0Voh6h7r"
      },
      "outputs": [],
      "source": [
        "# Inspect the DataFrame with Fake News to understand the given data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dwcty-wmJrFw"
      },
      "outputs": [],
      "source": [
        "# Print the column details for True News DataFrame\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EPLAnUMjjzQ4"
      },
      "outputs": [],
      "source": [
        "# Print the column details for Fake News Dataframe\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dyuDFzPkI67B"
      },
      "outputs": [],
      "source": [
        "# Print the column names of both DataFrames\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2fff1S7hq5h"
      },
      "source": [
        "### **1.1** Add new column  <font color = red>[3 marks]</font> <br>\n",
        "\n",
        "Add new column `news_label` to both the DataFrames and assign labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YOu_KhbH78du"
      },
      "outputs": [],
      "source": [
        "# Add a new column 'news_label' to the true news DataFrame and assign the label \"1\" to indicate that these news are true\n",
        "\n",
        "# Add a new column 'news_label' to the fake news DataFrame and assign the label \"0\" to indicate that these news are fake\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UShuo3h54DAh"
      },
      "source": [
        "### **1.2** Merge DataFrames  <font color = red>[2 marks]</font> <br>\n",
        "\n",
        "Create a new Dataframe by merging True and Fake DataFrames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5GX_QMy04M4-"
      },
      "outputs": [],
      "source": [
        "# Combine the true and fake news DataFrames into a single DataFrame\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FYCtKXD1JrFw"
      },
      "outputs": [],
      "source": [
        "# Display the first 5 rows of the combined DataFrame to verify the result\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IphUaBu3oFZK"
      },
      "source": [
        "### **1.3** Handle the null values  <font color = red>[2 marks]</font> <br>\n",
        "\n",
        "Check for null values and handle it by imputation or dropping the null values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4FRDxOo6r51j"
      },
      "outputs": [],
      "source": [
        "# Check Presence of Null Values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eiwvoyfYr6EB"
      },
      "outputs": [],
      "source": [
        "# Handle Rows with Null Values\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDyPvMITooWA"
      },
      "source": [
        "### **1.4** Merge the relevant columns and drop the rest from the DataFrame  <font color = red>[3 marks]</font> <br>\n",
        "\n",
        "Combine the relevant columns into a new column `news_text` and then drop irrelevant columns from the DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u9VI48jS_HTy"
      },
      "outputs": [],
      "source": [
        "# Combine the relevant columns into a new column 'news_text' by joining their values with a space\n",
        "\n",
        "# Drop the irrelevant columns from the DataFrame as they are no longer needed\n",
        "\n",
        "# Display the first 5 rows of the updated DataFrame to check the result\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L944HZpsJrFy"
      },
      "source": [
        "## **2.** Text Preprocessing <font color = red>[15 marks]</font> <br>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "On all the news text, you need to:\n",
        "<ol type=1>\n",
        "  <li> Make the text lowercase\n",
        "  <li> Remove text in square brackets\n",
        "  <li> Remove punctuation\n",
        "  <li> Remove words containing numbers\n",
        "</ol>\n",
        "\n",
        "\n",
        "Once you have done these cleaning operations you need to perform POS tagging and lemmatization on the cleaned news text, and remove all words that are not tagged as NN or NNS."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-6VW3V3jx1A"
      },
      "source": [
        "### **2.1** Text Cleaning  <font color = red>[5 marks]</font> <br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78OZs7P4kp41"
      },
      "source": [
        "#### 2.1.0 Create a new DataFrame to store the processed data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uXnN7aa_JrF0"
      },
      "outputs": [],
      "source": [
        "# Create a DataFrame('df_clean') that will have only the cleaned news text and the lemmatized news text with POS tags removed\n",
        "\n",
        "# Add 'news_label' column to the new dataframe for topic identification\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsf00J83mdNL"
      },
      "source": [
        "#### 2.1.1 Write the function to clean the text and remove all the unnecessary elements  <font color = red>[4 marks]</font> <br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qm7SjjSkJrFz"
      },
      "outputs": [],
      "source": [
        "# Write the function here to clean the text and remove all the unnecessary elements\n",
        "\n",
        "# Convert to lower case\n",
        "\n",
        "# Remove text in square brackets\n",
        "\n",
        "# Remove punctuation\n",
        "\n",
        "# Remove words with numbers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDFMNnrdkUvd"
      },
      "source": [
        "#### 2.1.2  Apply the function to clean the news text and store the cleaned text in a new column within the new DataFrame. <font color = red>[1 mark]</font> <br>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nOiDVvEIJrF0"
      },
      "outputs": [],
      "source": [
        "# Apply the function to clean the news text and remove all unnecessary elements\n",
        "# Store it in a separate column in the new DataFrame\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSAVnSelkF9d"
      },
      "source": [
        "### **2.2** POS Tagging and Lemmatization  <font color = red>[10 marks]</font> <br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCqNHDL2mHok"
      },
      "source": [
        "#### 2.2.1 Write the function for POS tagging and lemmatization, filtering stopwords and keeping only NN and NNS tags <font color = red>[8 marks]</font> <br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zgOu8t8HJrFz"
      },
      "outputs": [],
      "source": [
        "# Write the function for POS tagging and lemmatization, filtering stopwords and keeping only NN and NNS tags\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_T3So7PXlibT"
      },
      "source": [
        "#### 2.2.2  Apply the POS tagging and lemmatization function to cleaned text and store it in a new column within the new DataFrame. <font color = red>[2 mark]</font> <br>\n",
        "\n",
        "**NOTE: Store the cleaned text and the lemmatized text with POS tags removed in separate columns within the new DataFrame.**\n",
        "\n",
        "**This will be useful for analysing character length differences between cleaned text and lemmatized text with POS tags removed during EDA.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L9FWmibNI67F"
      },
      "outputs": [],
      "source": [
        "# Apply POS tagging and lemmatization function to cleaned text\n",
        "# Store it in a separate column in the new DataFrame\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMc5kjeqnX9b"
      },
      "source": [
        "### Save the Cleaned data as a csv file (Recommended)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fTjNG5xfnlwm"
      },
      "outputs": [],
      "source": [
        "## Recommended to perform the below steps to save time while rerunning the code\n",
        "# df_clean.to_csv(\"clean_df.csv\", index=False)\n",
        "# df_clean = pd.read_csv(\"clean_df.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0M0LseVjneMv"
      },
      "outputs": [],
      "source": [
        "# Check the first few rows of the DataFrame\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDZq_T3FYuOi"
      },
      "outputs": [],
      "source": [
        "# Check the dimensions of the DataFrame\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ah1aJPmiAqWz"
      },
      "outputs": [],
      "source": [
        "# Check the number of non-null entries and data types of each column\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMzqKme_2QQ0"
      },
      "source": [
        "## **3.** Train Validation Split <font color = red>[5 marks]</font> <br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jmx_W7Ty2PlK"
      },
      "outputs": [],
      "source": [
        "# Import Train Test Split and split the DataFrame into 70% train and 30% validation data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7Un1AElJrF2"
      },
      "source": [
        "## **4.** Exploratory Data Analysis on Training Data  <font color = red>[40 marks]</font> <br>\n",
        "\n",
        "Perform EDA on cleaned and preprocessed texts to get familiar with the training data by performing the tasks given below:\n",
        "\n",
        "<ul>\n",
        "  <li> Visualise the training data according to the character length of cleaned news text and lemmatized news text with POS tags removed\n",
        "  <li> Using a word cloud, find the top 40 words by frequency in true and fake news separately\n",
        "  <li> Find the top unigrams, bigrams and trigrams by frequency in true and fake news separately\n",
        "</ul>\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOZ7yZ4Fp1cp"
      },
      "source": [
        "### **4.1** Visualise character lengths of cleaned news text and lemmatized news text with POS tags removed  <font color = red>[10 marks]</font> <br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCI4DbZWpQ7n"
      },
      "source": [
        "##### 4.1.1  Add new columns to calculate the character lengths of the processed data columns  <font color = red>[3 marks]</font> <br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HifHm5rxpaxZ"
      },
      "outputs": [],
      "source": [
        "# Add a new column to calculate the character length of cleaned news text\n",
        "\n",
        "# Add a new column to calculate the character length of lemmatized news text with POS tags removed\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1JvT8lNpbFe"
      },
      "source": [
        "##### 4.1.2  Create Histogram to visualise character lengths  <font color = red>[7 marks]</font> <br>\n",
        "\n",
        " Plot both distributions on the same graph for comparison and to observe overlaps and peak differences to understand text preprocessing's impact on text length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-zaqJF6JrF2"
      },
      "outputs": [],
      "source": [
        "# Create a histogram plot to visualise character lengths\n",
        "\n",
        "# Add histogram for cleaned news text\n",
        "\n",
        "# Add histogram for lemmatized news text with POS tags removed\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9jD_6SeJrF3"
      },
      "source": [
        "### **4.2** Find and display the top 40 words by frequency among true and fake news in Training data after processing the text  <font color = red>[10 marks]</font> <br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n320yzDiEUH4"
      },
      "source": [
        "##### 4.2.1 Find and display the top 40 words by frequency among true news in Training data after processing the text  <font color = red>[5 marks]</font> <br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QcfdvtfZJrF3"
      },
      "outputs": [],
      "source": [
        "## Use a word cloud find the top 40 words by frequency among true news in the training data after processing the text\n",
        "\n",
        "# Filter news with label 1 (True News) and convert to it string and handle any non-string values\n",
        "\n",
        "# Generate word cloud for True News\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbHtjWzbEj__"
      },
      "source": [
        "##### 4.2.2 Find and display the top 40 words by frequency among fake news in Training data after processing the text  <font color = red>[5 marks]</font> <br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOFoDEscEkMO"
      },
      "outputs": [],
      "source": [
        "## Use a word cloud find the top 40 words by frequency among fake news in the training data after processing the text\n",
        "\n",
        "# Filter news with label 0 (Fake News) and convert to it string and handle any non-string values\n",
        "\n",
        "# Generate word cloud for Fake News\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DfCSbbmJrF4"
      },
      "source": [
        "### **4.3** Find and display the top unigrams, bigrams and trigrams by frequency in true news and fake news after processing the text  <font color = red>[20 marks]</font> <br>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApGagSppsAyL"
      },
      "source": [
        "##### 4.3.1 Write a function to get the specified top n-grams  <font color = red>[4 marks]</font> <br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5mbk5DS5JrF4"
      },
      "outputs": [],
      "source": [
        "# Write a function to get the specified top n-grams\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHcBL7vRsM4I"
      },
      "source": [
        "##### 4.3.2 Handle the NaN values  <font color = red>[1 mark]</font> <br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Ks69UQGCXpw"
      },
      "outputs": [],
      "source": [
        "# Handle NaN values in the text data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caioIgIEsfh2"
      },
      "source": [
        "### For True News\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYB2ZXZ83fjo"
      },
      "source": [
        "##### 4.3.3 Display the top 10 unigrams by frequency in true news and plot them as a bar graph  <font color = red>[2.5 marks]</font> <br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YX7fedm1JrF8"
      },
      "outputs": [],
      "source": [
        "# Print the top 10 unigrams by frequency in true news and plot the same using a bar graph\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3Q7wQnnsvOE"
      },
      "source": [
        "##### 4.3.4 Display the top 10 bigrams by frequency in true news and plot them as a bar graph  <font color = red>[2.5 marks]</font> <br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aV7kD7w8JrF8"
      },
      "outputs": [],
      "source": [
        "# Print the top 10 bigrams by frequency in true news and plot the same using a bar graph\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opdM04-Bs_Bg"
      },
      "source": [
        "##### 4.3.5 Display the top 10 trigrams by frequency in true news and plot them as a bar graph  <font color = red>[2.5 marks]</font> <br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xkh7vtbtJrF-"
      },
      "outputs": [],
      "source": [
        "# Print the top 10 trigrams by frequency in true news and plot the same using a bar graph\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prNx2Sm0WGEj"
      },
      "source": [
        "### For Fake News\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obBKlBVK3mfX"
      },
      "source": [
        "##### 4.3.6 Display the top 10 unigrams by frequency in fake news and plot them as a bar graph  <font color = red>[2.5 marks]</font> <br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1qKDzoSIWGXp"
      },
      "outputs": [],
      "source": [
        "# Print the top 10 unigrams by frequency in fake news and plot the same using a bar graph\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSlMnmqcYsNw"
      },
      "source": [
        "##### 4.3.7 Display the top 10 bigrams by frequency in fake news and plot them as a bar graph  <font color = red>[2.5 marks]</font> <br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r5xUk_r9Wa0f"
      },
      "outputs": [],
      "source": [
        "# Print the top 10 bigrams by frequency in fake news and plot the same using a bar graph\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i08WYIwPYs6R"
      },
      "source": [
        "##### 4.3.8 Display the top 10 trigrams by frequency in fake news and plot them as a bar graph  <font color = red>[2.5 marks]</font> <br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6nT9a1EvWa-s"
      },
      "outputs": [],
      "source": [
        "# Print the top 10 trigrams by frequency in fake news and plot the same using a bar graph\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VsOO_oHN8-_"
      },
      "source": [
        "## **5.** Exploratory Data Analysis on Validation Data [Optional]\n",
        "\n",
        "Perform EDA on validation data to differentiate EDA on training data with EDA on validation data and the tasks are given below:\n",
        "\n",
        "<ul>\n",
        "  <li> Visualise the data according to the character length of cleaned news text and lemmatized text with POS tags removed\n",
        "  <li> Using a word cloud find the top 40 words by frequency in true and fake news separately\n",
        "  <li> Find the top unigrams, bigrams and trigrams by frequency in true and fake news separately\n",
        "</ul>\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNkif5wrONfI"
      },
      "source": [
        "### **5.1** Visualise character lengths of cleaned news text and lemmatized news text with POS tags removed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAhwSWBWOTIj"
      },
      "source": [
        "##### 5.1.1  Add new columns to calculate the character lengths of the processed data columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6hYYTrdHORAs"
      },
      "outputs": [],
      "source": [
        "# Add a new column to calculate the character length of cleaned news text\n",
        "\n",
        "# Add a new column to calculate the character length of lemmatized news text with POS tags removed\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRILiD8hOX5q"
      },
      "source": [
        "##### 5.1.2  Create Histogram to visualise character lengths\n",
        "\n",
        "Plot both distributions on the same graph for comparison and to observe overlaps and peak differences to understand text preprocessing's impact on text length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j7LLOXC0Oaix"
      },
      "outputs": [],
      "source": [
        "# Create a histogram plot to visualise character lengths\n",
        "\n",
        "# Add histogram for cleaned news text\n",
        "\n",
        "# Add histogram for lemmatized news text with POS tags removed\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPxa0JrvOwFu"
      },
      "source": [
        "### **5.2** Find and display the top 40 words by frequency among true and fake news after processing the text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOZRD8vaFt2h"
      },
      "source": [
        "##### 5.2.1  Find and display the top 40 words by frequency among true news in validation data after processing the text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_9UVs4WOw0_"
      },
      "outputs": [],
      "source": [
        "## Use a word cloud find the top 40 words by frequency among true news after processing the text\n",
        "\n",
        "# Generate word cloud for True News\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6AQPITsFuop"
      },
      "source": [
        "##### 5.2.2  Find and display the top 40 words by frequency among fake news in validation data after processing the text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4gk6xTdSFu1X"
      },
      "outputs": [],
      "source": [
        "## Use a word cloud find the top 40 words by frequency among fake news after processing the text\n",
        "\n",
        "# Generate word cloud for Fake News\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dx0s2a7xOxKf"
      },
      "source": [
        "### **5.3** Find and display the top unigrams, bigrams and trigrams by frequency in true news and fake news after processing the text  \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2xZKU0RO5ft"
      },
      "source": [
        "##### 5.3.1 Write a function to get the specified top n-grams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g70ZFxcoOxS2"
      },
      "outputs": [],
      "source": [
        "## Write a function to get the specified top n-grams\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8ud5-r7O7ut"
      },
      "source": [
        "##### 5.3.2 Handle the NaN values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vsBu-aotPBJF"
      },
      "outputs": [],
      "source": [
        "## First handle NaN values in the text data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KVWcxoDPAiE"
      },
      "source": [
        "### For True News\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Am3uuIlU4wj1"
      },
      "source": [
        "\n",
        "##### 5.3.3 Display the top 10 unigrams by frequency in true news and plot them as a bar graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKdpz-XmPGHD"
      },
      "outputs": [],
      "source": [
        "## Print the top 10 unigrams by frequency in true news and plot the same using a bar graph\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiKrOL8rPLAs"
      },
      "source": [
        "##### 5.3.4 Display the top 10 bigrams by frequency in true news and plot them as a bar graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iuyYGnaNPLwq"
      },
      "outputs": [],
      "source": [
        "## Print the top 10 bigrams by frequency in true news and plot the same using a bar graph\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f8_h-BiPOKb"
      },
      "source": [
        "##### 5.3.5 Display the top 10 trigrams by frequency in true news and plot them as a bar graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8lz-XS7qPQZj"
      },
      "outputs": [],
      "source": [
        "## Print the top 10 trigrams by frequency in true news and plot the same using a bar graph\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MfApeGrd6Fl"
      },
      "source": [
        "### For Fake News"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CH1csmZeGqh"
      },
      "source": [
        "##### 5.3.6 Display the top 10 unigrams by frequency in fake news and plot them as a bar graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w310WzGAeG4K"
      },
      "outputs": [],
      "source": [
        "## Print the top 10 unigrams by frequency in fake news and plot the same using a bar graph\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFjOaPx7eHFw"
      },
      "source": [
        "##### 5.3.7 Display the top 10 bigrams by frequency in fake news and plot them as a bar graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZuTqnjkIeHSJ"
      },
      "outputs": [],
      "source": [
        "## Print the top 10 bigrams by frequency in fake news and plot the same using a bar graph\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vn_oiixheHf_"
      },
      "source": [
        "##### 5.3.8 Display the top 10 trigrams by frequency in fake news and plot them as a bar graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xduyO4gheHtI"
      },
      "outputs": [],
      "source": [
        "## Print the top 10 trigrams by frequency in fake news and plot the same using a bar graph\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-I0k0QtJrGA"
      },
      "source": [
        "## **6.** Feature Extraction  <font color = red>[10 marks]</font> <br>\n",
        "\n",
        "For any ML model to perform classification on textual data, you need to convert it to a vector form. In this assignment, you will use the Word2Vec Vectorizer to create vectors from textual data. Word2Vec model captures the semantic relationship between words.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09xy3mAbtZgZ"
      },
      "source": [
        "### **6.1** Initialise Word2Vec model  <font color = red>[2 marks]</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y8fGwaCPJrGA",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "## Write your code here to initialise the Word2Vec model by downloading \"word2vec-google-news-300\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYzD85nTJrGA"
      },
      "source": [
        "### **6.2** Extract vectors for cleaned news data   <font color = red>[8 marks]</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ffzdDpp_JrGB"
      },
      "outputs": [],
      "source": [
        "## Write your code here to extract the vectors from the Word2Vec model for both training and validation data\n",
        "\n",
        "\n",
        "## Extract the target variable for the training data and validation data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Q9lwvNEJrGB"
      },
      "source": [
        "## **7.** Model Training and Evaluation <font color = red>[45 marks]</font>\n",
        "\n",
        "You will use a set of supervised models to classify the news into true or fake."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sO16REK-xpq4"
      },
      "source": [
        "### **7.0** Import models and evaluation metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0BKoT3wxpq4"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLuHH1olZXQq"
      },
      "source": [
        "### **7.1** Build Logistic Regression Model  <font color = red>[15 marks]</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1ItmvvGwduv"
      },
      "source": [
        "##### 7.1.1 Create and train logistic regression model on training data  <font color = red>[10 marks]</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wzXd3YSu5eyY"
      },
      "outputs": [],
      "source": [
        "## Initialise Logistic Regression model\n",
        "\n",
        "## Train Logistic Regression model on training data\n",
        "\n",
        "## Predict on validation data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvNKC8ob8IAL"
      },
      "source": [
        "##### 7.1.2 Calculate and print accuracy, precision, recall and f1-score on validation data <font color = red>[5 marks]</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BEyQcSoWo4xs"
      },
      "outputs": [],
      "source": [
        "## Calculate and print accuracy, precision, recall, f1-score on predicted labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6P4MA_AC216"
      },
      "outputs": [],
      "source": [
        "# Classification Report\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRGPMQZd8r8B"
      },
      "source": [
        "### **7.2** Build Decision Tree Model <font color = red>[15 marks]</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rgv4vqrt81sH"
      },
      "source": [
        "##### 7.2.1 Create and train a decision tree model on training data <font color = red>[10 marks]</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-mTab94xpq4"
      },
      "outputs": [],
      "source": [
        "## Initialise Decision Tree model\n",
        "\n",
        "## Train Decision Tree model on training data\n",
        "\n",
        "## Predict on validation data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZ_Vj5fs6w9I"
      },
      "source": [
        "##### 7.2.2 Calculate and print accuracy, precision, recall and f1-score on validation data <font color = red>[5 marks]</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15iYiCQhp7jo"
      },
      "outputs": [],
      "source": [
        "## Calculate and print accuracy, precision, recall, f1-score on predicted labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DohNckxxxpq4"
      },
      "outputs": [],
      "source": [
        "# Classification Report\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnB_P9kd9EdC"
      },
      "source": [
        "### **7.3** Build Random Forest Model <font color = red>[15 marks]</font>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhW0nyU29In9"
      },
      "source": [
        "##### 7.3.1 Create and train a random forest model on training data <font color = red>[10 marks]</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uIvY9-oPxpq4"
      },
      "outputs": [],
      "source": [
        "## Initialise Random Forest model\n",
        "\n",
        "## Train Random Forest model on training data\n",
        "\n",
        "## Predict on validation data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRBxZieM7eea"
      },
      "source": [
        " ##### 7.3.2 Calculate and print accuracy, precision, recall and f1-score on validation data <font color = red>[5 marks]</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VzNK3jcCq01-"
      },
      "outputs": [],
      "source": [
        "## Calculate and print accuracy, precision, recall, f1-score on predicted labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VIBCo_kFxpq4"
      },
      "outputs": [],
      "source": [
        "# Classification Report\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **8.** Conclusion <font color = red>[5 marks]</font>\n",
        "\n",
        "Summarise your findings by discussing patterns observed in true and fake news and how semantic classification addressed the problem. Highlight the best model chosen, the evaluation metric prioritised for the decision, and assess the approach and its impact."
      ],
      "metadata": {
        "id": "Lnj7RUDDSEJX"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}